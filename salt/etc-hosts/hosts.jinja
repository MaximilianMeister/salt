{%- set this_id       = salt['grains.get']('id') %}
{%- set this_hostname = salt['grains.get']('localhost', '') %}
{%- set this_roles    = salt['grains.get']('roles', []) %}

{%- set internal_fqdn = "api api." + pillar['internal_infra_domain'] -%}
{%- set external_fqdn = "" -%}
{%- if not salt['caasp_filters.is_ip'](pillar['api']['server']['external_fqdn']) -%}
{%- set external_fqdn = pillar['api']['server']['external_fqdn'] -%}
{%- endif -%}

{%- macro nodes_entries(expr) -%}
  {# note regarding node removals:
   # we need the "node_(addition|removal)_in_progress" nodes here, otherwise
   #   - nodes being removed will be immediately banned from the cluster (with a message like:
   #    'rejected connection from <NODE> (error tls: <NODE-IP> does not match any of DNSNames [...]')
   #    and the cluster will become unhealthy
   #   - nodes being added will not be able to join (with some similar TLS verification error)
   # doing another /etc/hosts update just for one stale entry seem like an overkill,
   # so the /etc/hosts cleanup will have to be delayed for some other moment...
   #}
  {%- set nodes = salt['caasp_nodes.get_with_expr'](expr, grain='network.interfaces') %}
  {%- for id, ifaces in nodes.items() %}
    {%- set ip = salt['caasp_net.get_primary_ip'](host=id, ifaces=ifaces) %}
    {%- if ip|length > 0 %}
      {%- set nodename = salt['caasp_net.get_nodename'](host=id) %}
      {%- if nodename|length > 0 %}
        {%- set ns = nodename + ' ' +
                   nodename + '.' + pillar['internal_infra_domain'] + ' ' +
                   id + ' ' +
                   id + '.' + pillar['internal_infra_domain'] %}

{{ ip }} {{ ns }}

      {%- endif %}
    {%- endif %}

    {%- if id == this_id %}
# try to make Salt happy by adding an ipv6 entry
# for the local host (not really used for anything else)
::1 {{ ns }} {{ this_hostname }}
    {%- endif %}
  {%- endfor %}
{%- endmacro %}

{##################################################################}

### service names ###
# set the apiserver for 127.0.0.1 on all hosts as haproxy is listening on all nodes
# and forwarding connections to the real master
{%- if "kube-master" in this_roles %}
127.0.0.1 {{ internal_fqdn }} {{ external_fqdn }}
{%- elif "admin" in this_roles %}
127.0.0.1 {{ internal_fqdn }} {{ external_fqdn }} ldap.{{ pillar['internal_infra_domain'] }}
{%- else %}
127.0.0.1 {{ internal_fqdn }}
{%- endif %}

### admin nodes ###
{{ nodes_entries('G@roles:admin') }}

### kubernetes masters ###
{{ nodes_entries('G@roles:kube-master') }}

### kubernetes workers ###
{{ nodes_entries('G@roles:kube-minion') }}

### other machines in the cluster (ie, etcd nodes, unassigned machines...) ###
{{ nodes_entries('not ( P@roles:(admin|ca) or P@roles:kube-(master|minion) )') }}
